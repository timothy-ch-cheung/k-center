As identified by \textcite{anegg_technique_2020}, most of the generalisations of the $k$-center problem fall under one of two categories:
\begin{itemize}
    \item constraints on which points can be selected as centers
    \item constraints on the points which must be covered
\end{itemize}
 The Fair Robust $k$-center (\cite{harris_lottery_2017}) is an example of the former, it highlighted that the robust $k$-center problem could be biased towards always excluding the same outliers. Their formulation of the Fair Robust $k$-center uses a lottery model which adds individual probability guarantees to each vertex will be covered. Their results showed a pseudo approximation which opened at most $k+1$ centers, which was later improved upon by \citeauthor{anegg_technique_2020} to a true 4-approximation.

The colourful $k$-center falls under the latter. Another example of the latter is the fair $k$-center (\cite{kleindessner_fair_2019}), the clients are split into demographic groups (similarly to the colorful $k$-center), and a constraint on the number of centers which can be chosen from each group is introduced.

The notion of fairness in the wider field of machine learning (\acrshort{ml}) has been well established, \textcite{mehrabi_survey_2019} conducted a comprehensive review of bias in \acrshort{ml}. Their work covered real world cases where \acrshort{ml} had been unfairly biased, with a detailed breakdown of different types of bias and an overview the algorithms proposed for fair \acrshort{ml}.