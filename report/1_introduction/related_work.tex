As identified by \textcite{anegg_technique_2020}, most of the generalisations of the $k$-center problem fall under one of two categories:
\begin{itemize}
    \item constraints on which points can be selected as centers
    \item constraints on the points which must be covered
\end{itemize}
While The colourful $k$-center falls under the latter, however it is not the only variant in that category. \textcite{harris_lottery_2017} introduced the Fair Robust $k$-center problem, where they highlighted that the robust $k$-center problem could be biased towards always excluding the same outliers. Their formulation of the Fair Robust $k$-center uses a lottery model which adds individual probability guarantees to each vertex will be covered while ensuring that a solution will minimise the radius. Their results showed a pseudo approximation which opened at most $k+1$ centers, which was later improved upon by \textcite{anegg_technique_2020} to a true 4-approximation.

There has also been work on $k$-center with fairness constraints on which points can be selected as centers, the work on data summarisation by \textcite{kleindessner_fair_2019} is an example of this. They described the Fair $k$-center; the clients are split into demographic groups (similarly to the colorful $k$-center), a constraint on the number of centers which can be chosen from each demographic is introduced. Their results revealed a constant factor approximation algorithm, with the specific approximation factor being dependent on the number of demographic groups. 

The notion of fairness in the wider field of machine learning has been well established, \textcite{mehrabi_survey_2019} conducted a comprehensive review of bias in machine learning. Their work covered real world cases where machine learning had been unfairly biased towards certain demographics, with a detailed breakdown of different types of unfairness/bias and an overview the literature and algorithms proposed for fair machine learning.