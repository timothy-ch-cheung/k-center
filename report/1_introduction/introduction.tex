In the recent years the term Big Data has become more relevant, it is defined as data which is too large to process using traditional methods. Even in 2008, Google processed over 20 petabytes of data per day (\cite{dean_mapreduce_2008}). There is a strong motivation for improving the techniques used in handling Big Data, since it has real world applications in fields such as medicine, transportation and city planning (\cite{al_nuaimi_applications_2015,obermeyer_predicting_2016}). 

Machine learning is often applied to analyse and organise these large sets of data. In the realm of machine learning, the two overarching paradigms for processing data are supervised and unsupervised learning. Supervised learning involves data sets which are labelled with the ground truth, whereas unsupervised learning does not require labelled data. Classification is a supervised learning method where given a set of categories, we make use of the labelled data set with the goal of creating a function which predicts the category of an unseen example. Conversely, clustering attempts to group together similar data points together. 

There are several approaches to clustering, including centroid, density and connectivity based models. In centroid based models, the data points are partitioned based on specific points designated as centers; each data point is assigned to a center to minimise some dissimilarity metric. Some common centroid based clustering methods are $k$-means, $k$-median, and $k$-center. These clustering problems NP-hard, which means given that $P\neq NP$, there does not exist an algorithm which runs in polynomial time which also produces a perfectly optimal solution. As generating clusters poses considerable complexity and computational cost, there is interest in the development of new methods for traditional clustering problems (\cite{zhao_parallel_1970}). 

In this paper we explore centroid based clustering, specifically the $k$-center and its variations. The $k$-center problem is a classical \gls{combinatorial_optimisation} problem first proposed by \textcite{hakimi_optimum_1964}. Its goal is to cluster points together such that the distance between the point farthest from its nearest center is minimised. In the literature for solving the k-center problem there are broadly three types of algorithms: \gls{exact_algs} (which give an optimal solution), \gls{approx_algs} (which give a guarantee of solution cost) and \glspl{metaheuristic} (which provide no such guarantee).

The $k$-center problem has been traditionally studied with the requirement that all points must be part of a cluster, \textcite{charikar_algorithms_2001} identified that a small amount of outliers can cause a large increase in the optimal cost. This resulted in them introducing the robust $k$-center problem which considers the scenario where a minimum number of points must be clustered (rather than all of them). Consequently, further generalisations of the Robust $k$-center problem have been formulated, the colorful $k$-center problem divides the points in colour groups and the minimum number of points from each colour group which must be part of a cluster is specified (\cite{bandyapadhyay_constant_2019}). This work is part of a larger trend of investigating bias and fairness in traditional machine learning approaches (\cite{mehrabi_survey_2019, anegg_technique_2020}).

The $k$-center problem has been typically applied to facility location problems, where we are given a set of clients and we have to allocate them to $k$ facilities to minimise the distance of the farthest candidate to its nearest facility. However, it has also seen relevance in more a novel applications, such as rendering images with natural lighting (\cite{agarwal_structured_2003}) and summarising a collection of images subject to fairness constraints (\cite{kleindessner_fair_2019}).

This project will explore the various approaches to solving the $k$-center and colourful $k$-center problems.

%\input{1_introduction/structure_summary}