In the recent years the term Big Data has become more relevant, it is defined as data which is too large to process using traditional methods. Even in 2008, Google processed over 20 petabytes of data per day (\cite{dean_mapreduce_2008}). There is a strong motivation for improving the techniques used in handling Big Data, since it has real world applications in fields such as medicine, transportation and city planning (\cite{al_nuaimi_applications_2015,obermeyer_predicting_2016}). 

Machine learning is often applied to analyse and organise these large sets of data. In the realm of machine learning, the two overarching paradigms for processing data are supervised and unsupervised learning. Supervised learning involves data sets which are labelled with the ground truth, whereas unsupervised learning does not require labelled data. Classification is a supervised learning method where given a set of categories, we make use of the labelled data set with the goal of creating a function which predicts the category of an unseen example. On the other hand, clustering attempts to group together similar data points together. 

There are several approaches to clustering, including centroid, density and connectivity based models. In centroid based models, the data points are partitioned based on specific points designated as centers; each data point is assigned to a center to minimise some dissimilarity metric. Some common centroid based clustering methods are $k$-means, $k$-median, and $k$-center. These clustering problems NP-hard, which means given that $P\neq NP$, there does not exist an algorithm which runs in polynomial time which also produces a perfectly optimal solution. As generating clusters poses considerable complexity and computational cost, there is interest in the development of new methods to traditional clustering solving problems such as $k$-means (\cite{zhao_parallel_1970}). 

In this paper we explore a specific method of centroid based clustering, the $k$-center and its variations. The $k$-center problem is a classical combinatorial optimisation problem first proposed by \textcite{hakimi_optimum_1964}. Its goal is to cluster points together such that the distance between the point farthest from its nearest center is minimised. In the literature for solving the k-center problem there are broadly two approaches: approximation algorithms (which guarantee a solution with a cost within a multiplicative factor of the optimal cost) and local search algorithms (which provide no such guarantee). \textcite{gonzalez_clustering_1985} and \textcite{hochbaum_best_1985} both proposed algorithms that give solutions to the $k$-center problem with a cost within twice the optimal.

The $k$-center problem has been traditionally studied with the assumption that all points must be part of a cluster, \textcite{charikar_algorithms_2001} identified that a small amount of outliers can cause a large increase in the optimal cost. This resulted in them introducing the Robust $k$-center problem which considers the scenario where a minimum number of points must be clustered (rather than all of them). Since then further generalisations of the Robust $k$-center problem have been formulated, the Colorful $k$-center problem divides the points in colour groups and the minimum number of points from each colour group which must be part of a cluster is specified (\cite{bandyapadhyay_constant_2019}). The work by \textcite{bandyapadhyay_constant_2019} is part of a larger trend of investigating bias and fairness in traditional machine learning approaches (\cite{mehrabi_survey_2019, anegg_technique_2020}).

The $k$-center problem has been typically applied to facility location problems, where we are given a set of clients and we have to allocate them to $k$ facilities to minimise the distance of the farthest candidate to its nearest facility. However, it has also seen relevance in more a novel applications, such as rendering images with natural lighting (\cite{agarwal_structured_2003}) and summarising a collection of images subject to fairness constraints (\cite{kleindessner_fair_2019}).

The remainder of the paper is structured as follows. The following section is a review of the literature on the $k$-center problem and its variants and descriptions the algorithms at a high level (\cref{section:lit_review}). \Cref{section:background} is a detailed description of the algorithms which we have implemented for the $k$-center problem and Colourful $k$-center problem. In \cref{section:design} we describe the design of our tool to to visualise and understand these algorithms in section. \Cref{section:results} evaluates the performance of the algorithm using empirical methods; in particular, we investigate the running time and analyse the distance between the solution cost to the optimal cost. Finally in \cref{section:evaluation}, we provide evaluation of our algorithm and tool, then discuss potential extensions.