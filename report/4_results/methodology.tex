We conduct empirical experiments to evaluate the performance of the search based algorithm as they provide no theoretical guarantee of solution quality.

The search based algorithms (Plateau Surfer GRASP and PBS) we have considered are stochastic in nature and therefore do not produce consistent outcomes. In the context of genetic algorithms, \textcite{kramer_genetic_2017} recommends at least 25 but 50 or 100 is preferable. To evaluate their ability to solve the $k$-center problem, we run the \emph{Gon}, PBS and GRASP Plateau Surfer algorithms (described in \cref{subsection:approaches_k_center}) on the OR-LIB dataset; a data set originally designed for the $k$-median but later widely used for the $k$-center. The OR-LIB data set contains 40 problem instances ranging from 100-900 nodes, we evaluate the algorithms by comparing the solution cost with the optimal cost (reported by \cite{pullan_memetic_2008}). To determine the statistical significance of our results, we conduct a Wilcoxon rank sum test on the cost of each solution. The Wilcoxon test compares if the difference between a set of pairs is statistically significant, a P-value below 0.05 is considered significant (\cite{wilcoxon_individual_1992}).

For evaluating the algorithm performance on the Colourful $k$-center problem, there does not exist an established data set which we know the optimal costs for. Therefore we conduct two sets of experiments to ensure our results are robust, both of which we perform the Wilcoxon ranked sum test on.

The first is to generate our own problem instances for the Colourful $k$-center problem. In this we generate problem instances ranging from 100-900 nodes. The motivation for this is that we can generate instances with known optimal costs.

The second experiment use a the Gowalla data set from the UCI Repeat Consumption Matrices. This data set represent the number of tweets with at a geolocation, we interpret the number of tweets as the demographic and the geolocation as coordinates for the clients/facilities. The motivation for this is to address any unknown bias we may have introduced by generating our own data set. In addition, we also brute force a small 40 node sample to compare the difference to the optimal cost on a real data set. We divide the data set into groups of 100-900 nodes, reusing the $k$ value from the ORLIB data set. The mean consumption for each group is calculated and locations above and below the mean are separated into two groups, the colour constraints is to cover at least 75\% of each group.

The experiments were conducted on an AMD 4.2GHz Ryzen 4800h and 16gb ram PC. 