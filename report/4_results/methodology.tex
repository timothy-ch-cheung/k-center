We conduct empirical experiments to evaluate the performance of each algorithm. \acrshort{grasp_ps}, \acrshort{pbs} and colourful PBS are stochastic and therefore have a degree of variability in solution cost. In the context of genetic algorithms, \textcite{kramer_genetic_2017} suggests that 50 trials is sufficient to address variance. We set a timeout for the $k$-center algorithms to $0.1n + 0.5k$ seconds and $0.15n + 0.5k$ for the colourful $k$-center algorithms (since it is harder). When benchmarking the \emph{Ban} algorithm, if it returned more than $k$-centers, we retried the test with $k-1$ as the parameter. For algorithms that produce multiple results (\acrshort{pbs}, \acrshort{grasp_ps}) we record the time at which the minimum cost is reached. All experiments were conducted on an AMD 4.2GHz Ryzen 4800h PC with 16gb ram. 

\paragraph{Data Sets}~\\
To evaluate the algorithms for the $k$-center we use the OR-LIB dataset (\cite{beasley_note_1985}), originally designed for the $k$-median but later widely used for the $k$-center. OR-LIB has 40 problem instances ranging from 100-900 nodes with known optimal solutions (\cite{pullan_memetic_2008}). 

As the colourful $k$-center problem was introduced very recently, it does not have any data sets. Generating our own data sets raises two issues; there may be bias in our data generation method and we may not know the optimal solution to our instances. Therefore we create two new data sets to address this (see \cref{appendix:test_datasets}):
\begin{enumerate}
    \item GOWALLA - To address the possibility of bias in synthetic data sets, we used a real world 3D data set. We take inspiration from the Gowalla UCI Repeat Consumption Matrices (\cite{kotzias_predicting_2019}), which is based on the number of times a user has checked in at a location. The UCI data set uses discretised geolocations, as this information is important to our problem we generate our own repeat consumption matrices (with latitude/longitude information) from the original Gowalla data set (\cite{cho_friendship_2011}). The locations can be split into blue/red by setting locations visited less frequently than average as blue and more frequently as red. The distance between two locations is defined by the Haversine distance. 
    \item SYNTHETIC - 40 problems of 100-900 nodes in a 2D plane with known optimal costs (\emph{opt}). Given an optimal cost $\rho$ and $k$, we generate $k$ centers, a uniform random set of inlier points $\leq\rho$ distance from a center, and a set of outlier points $>\rho$ from the centers. We generate 36 instances with 20\% of the vertices as outliers and 4 problem instances which have 80\% of vertices as outliers. The full procedure is described in \cref{appendix:generate_problem_instances}.
\end{enumerate}

\paragraph{Statistical Analysis Methods}~\\
To decide whether our results are significant we perform statistical tests. The null hypothesis is that the results come from the same distribution. The statistical test outputs a p-value, in order to reject the null hypothesis the p-value needs to fall below a significance level $\alpha$. We take $\alpha =0.05$, which means there is a 5\% risk of concluding our samples come from different distributions when they are not. We perform the Wilcoxon test for the colourful $k$-center and the Quade test for the $k$-center.

\newcounter{method_counter}
\setcounter{method_counter}{0}
\begin{figure}[H] 
    \addtocounter{figure}{-1}
    \renewcommand{\thefigure}{\arabic{method_counter}}
    \renewcommand\figurename{Method}
    \addtocounter{method_counter}{1}
    \caption{Wilcoxon (\cite{wilcoxon_individual_1945})}
    \label{method:wilcoxon_test}
    \vspace{0.1cm}
    \textbox{\small{\textbf{Wilcoxon signed rank test}\newline A statistical hypothesis test for two sets of paired samples. The general idea is to calculate the differences between pairs then rank the differences by size. The test statistic is calculated by summing the ranks multiplied by the sign of the differences.\newline \textbf{R function: wilcoxon.test(sampleA, sampleB, paired=true)}}}
\end{figure}

\begin{figure}[H] 
    \addtocounter{figure}{-1}
    \renewcommand{\thefigure}{\arabic{method_counter}}
    \renewcommand\figurename{Method}
    \addtocounter{method_counter}{1}
    \caption{Quade (\cite{conover_analysis_1982})}
    \label{method:quade_test}
    \vspace{0.1cm}
    \textbox{\small{\textbf{Quade omnibus test}\newline An extension of the Wilcoxon test for 3 or more sets of grouped samples. An omnibus test is used to determine if the variance between all samples are significant.\newline \textbf{R function: quade.test(matrix)}}}
\end{figure}

\begin{figure}[H] 
    \addtocounter{figure}{-1}
    \renewcommand{\thefigure}{\arabic{method_counter}}
    \renewcommand\figurename{Method}
    \addtocounter{method_counter}{1}
    \caption{Post-Hoc Quade (\cite{conover_analysis_1982})}
    \label{method:post_hoc_quade}
    \vspace{0.1cm}
    \textbox{\small{\textbf{Post-hoc Quade test}\newline If the Quade omnibus test returns a significant result we would like to determine which pairs of samples are significant. This is done using a post-hoc test, which returns p-values for each paired sample. \newline\textbf{R function: quadeAllPairsTest(matrix, dist="TDist", p.adjust.method="holm")}}}
\end{figure}

Given our results are statistically significant, we count the number of problem instances where one algorithm outperforms another to determine the better algorithm. 

\begin{figure}[H] 
    \addtocounter{figure}{-1}
    \renewcommand{\thefigure}{\arabic{method_counter}}
    \renewcommand\figurename{Method}
    \addtocounter{method_counter}{1}
    \caption{Counting results}
    \label{method:counting_results}
    \vspace{0.1cm}
    \textbox{\small{Given paired samples A and B, we count the number of problem instances where A has a lower mean cost ($\mu$) than B. If more than half of the problem instances have lower cost in sample A than B and a statistical test proves that the difference is significant, then we conclude A is better than B.}}
\end{figure}

\begin{figure}[H] 
    \addtocounter{figure}{-1}
    \renewcommand{\thefigure}{\arabic{method_counter}}
    \renewcommand\figurename{Method}
    \addtocounter{method_counter}{1}
    \caption{Counting results (accounting for variance)}
    \label{method:counting_results_variance}
    \vspace{0.1cm}
    \textbox{\small{We use the \gls{empirical_rule} to account to account for variance, it states that under a normal distribution under a normal distribution 95\% and 99.7\% of data will fall under 2 and 3 standard deviations from the mean respectively. This method is the same as \Cref{method:counting_results}, except we adjust the mean costs ($\mu$) by a parameter $\Delta\in\{2,3\}$ which represents the number of standard deviations. We increase the mean costs from sample A by $\Delta\sigma$ and modify the costs from sample B to be $max(opt, B_i -\Delta\sigma)$, $i\in$ problem\_instance (if $opt$ is unknown, set the cost to $B_i -\Delta\sigma$).\newline\newline If more than half of the problem instances have lower cost in sample A than B and a statistical test proves that the difference is significant, then we conclude A is better than B.}}
\end{figure}

\input{4_results/tables/method_used}